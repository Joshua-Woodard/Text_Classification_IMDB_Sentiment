{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_IMDB_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXtJXhV2jbEbp/PhjQY64j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joshua-Woodard/Text_Classification_IMDB_Sentiment/blob/main/Text_Classification_IMDB_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification"
      ],
      "metadata": {
        "id": "3AA9dY5ZazCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "# Get helper_functions.py script from course GitHub\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py \n",
        "\n",
        "# Import helper functions we're going to use\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir, calculate_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj4em3qBcZcb",
        "outputId": "e0e00b09-52f2-4836-911c-290f84c61692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-13 19:06:23--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.1’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-13 19:06:23 (65.8 MB/s) - ‘helper_functions.py.1’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data"
      ],
      "metadata": {
        "id": "ARhN67N0WO8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "imdb_dataset = pd.read_csv(\"/content/drive/MyDrive/IMDB_dataset.zip\")\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "64MNwaMK0JXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "807de882-30fa-4c8f-a8db-ba7d726c1b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-917fd7e8-a012-4dc6-a456-3db2589cf98e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-917fd7e8-a012-4dc6-a456-3db2589cf98e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-917fd7e8-a012-4dc6-a456-3db2589cf98e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-917fd7e8-a012-4dc6-a456-3db2589cf98e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "XoYOv9nRWQ7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create column transformer to both normalize/preprocess our data (one-hot-encodes too!)\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "zOr5exwDcn7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = imdb_dataset.drop([\"sentiment\"], axis=1)\n",
        "y = imdb_dataset[\"sentiment\"]"
      ],
      "metadata": {
        "id": "TGts0Y0ae6mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(), y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cB719SAeHZ4",
        "outputId": "cfa6eddb-a7a8-4903-852f-83967431ba19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                              review\n",
              " 0  One of the other reviewers has mentioned that ...\n",
              " 1  A wonderful little production. <br /><br />The...\n",
              " 2  I thought this was a wonderful way to spend ti...\n",
              " 3  Basically there's a family where a little boy ...\n",
              " 4  Petter Mattei's \"Love in the Time of Money\" is..., 0    positive\n",
              " 1    positive\n",
              " 2    positive\n",
              " 3    negative\n",
              " 4    positive\n",
              " Name: sentiment, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup train/test split"
      ],
      "metadata": {
        "id": "V7lz3NZFWVPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train/test split\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(X.to_numpy(), y.to_numpy(), test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "t6p_hzFleJ4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sentences), len(test_sentences), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjOWoSCFfxh_",
        "outputId": "dd1c3bfd-9eac-4c9b-9d8b-72fd98805efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 10000, 40000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-fBHsJygNbo",
        "outputId": "52881584-7740-4666-d2fc-12f632747011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'positive', 'negative', ..., 'positive', 'negative',\n",
              "       'positive'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_labels_encoded = le.fit_transform(y_train)\n",
        "test_labels_encoded = le.transform(y_test)"
      ],
      "metadata": {
        "id": "Lxco_dBygBAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_encoded, test_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgMe1DxNh8TB",
        "outputId": "b5764582-b184-4c01-f5ab-2a1800ebcf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 1, ..., 0, 1, 1]), array([1, 1, 0, ..., 1, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Rpjo0nh9q2",
        "outputId": "298ee8d2-5a40-4c1f-b9ff-5219dbd479a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['That\\'s what I kept asking myself during the many fights, screaming matches, swearing and general mayhem that permeate the 84 minutes. The comparisons also stand up when you think of the one-dimensional characters, who have so little depth that it is virtually impossible to care what happens to them. They are just badly written cyphers for the director to hang his multicultural beliefs on, a topic that has been done much better in other dramas both on TV and the cinema.<br /><br />I must confess, I\\'m not really one for spotting bad performances during a film, but it must be said that Nichola Burley (as the heroine\\'s slutty best friend) and Wasim Zakir (as the nasty, bullying brother) were absolutely terrible. I don\\'t know what acting school they graduated from, but if I was them I\\'d apply for a full refund post haste. Only Samina Awan in the lead role manages to impress in a cast of so-called British talent that we\\'ll probably never hear from again. At least, that\\'s the hope. Next time, hire a different scout.<br /><br />Another intriguing thought is the hideously fashionable soundtrack featuring the likes of Snow Patrol, Ian Brown and Keane. Now, I\\'m a bit of a music fan and I\\'m familiar with most of these artists output, but I didn\\'t recognise any of the tracks during this movie (apart from the omnipresent \"Run\"). B-sides, anyone? We get many, many musical montages which telegraph how we\\'re suppose to feel. These are accompanied by such startlingly original images as couples kissing by a swollen lake and canoodling in doorways. This is a problem, as none of the songs convey the mood efficiently, and we realise the director lacks the ability to carry the emotional journey to the audience through storytelling and dialogue alone.<br /><br />The ending is presumably meant to be just desserts, as everybody gets their comeuppance and there is at least one big shock in store.. But I remained resolutely unmoved because the script had given me no-one to root for. It\\'s not enough to tackle a hot-button issue, you have to actually give us a plot that hasn\\'t already been done to death and individuals who are more than window dressing. As it stands, this film is a noble failure, with only the promising lead actress and a few mildly diverting punch-ups to save it from the bin. 4/10. Must try harder..'],\n",
              "       [\"I did not watch the entire movie. I could not watch the entire movie. I stopped the DVD after watching for half an hour and I suggest anyone thinking of watching themselves it stop themselves before taking the disc out of the case.<br /><br />I like Mafia movies both tragic and comic but Corky Romano can only be described as a tragic attempt at a mafia comedy.<br /><br />The problem is Corky Romano simply tries too hard to get the audience to laugh, the plot seems to be an excuse for moving Chris Kattan (Corky) from one scene to another. Corky himself is completely overplayed and lacks subtlety or credulity - all his strange mannerisms come across as contrived - Chris Kattan is clearly 'acting' rather than taking a role - it bounces you right out of the story. Each scene is utterly predictable, the 'comedic event' that will occur on the set is obvious as soon as each scene is introduced. In comedies such as Mr. Bean the disasters caused by the title character are funny because you can empathise with the characters motivations and initial event and the situation the character ends up in is not telegraphed. Corky however gives the feeling that he is deliberately screwing up in a desperate attempt to draw a laugh from the audience.<br /><br />If Chris had not played such an alien character (who never really connects with the other characters in the movie) and whose behaviour is entirely inexplicable (except for trying to draw laughs) and the comedy scenes weren't so predictable and stereotyped - all the jokes seemed far too familiar) this movie could have been watchable. But it isn't. Don't watch it.\"],\n",
              "       [\"A touching love story reminiscent of \\x91In the Mood for Love'. Drawing heavily on Chinese poetry and how this is used by eastern people to communicate feelings to each other, the story focuses on a schoolteacher who wants so much to be a model teacher as well as a good husband and father. A senior student is very attracted to him. As the story unfolds we see the emotions below the surface in his 20 year marriage and how he grapples with the moral dilemmas that face him. A beautiful and moving story.\"],\n",
              "       [\"This latter-day Fulci schlocker is a totally abysmal concoction dealing with an incurable gambler (Brett Halsey) who decides Bluebeard-style to pay off his ever-rising debts by seducing some of the ugliest bitches you will ever lay your eyes on and who just happen to be wealthy widows! The Fulci-penned script also contrives to incorporate a few blackly comedic elements - which only result in some unfunny business involving a corpse which won't stay put, an opera singer victim who won't stop singing, etc. - not to mention a doppelganger theme straight out of THE STUDENT OF PRAGUE - although, in this case, the two personas communicate via pre-recorded radio messages!! In the end, I can't say I'm surprised that this film shows no sign of the sophistication of Mario Bava's HATCHET FOR THE HONEYMOON (1970) which it resembles in several ways and that it is content to merely pile up the disgustingly gory (but none-too-convincing) effects of dismembered limbs and squashed or melting faces with which, alas, Fulci had by then become completely associated.\"],\n",
              "       [\"First of all, I firmly believe that Norwegian movies are continually getting better. From the tedious emotional films of the 70's and 80's, movies from this place actually started to contain a bit of humour. Imagine.. Actual comedies were made! Movies were actually starting to get entertaining and funny, as opposed to long, dark, depressing and boring.<br /><br />During the 90's and 00's several really great movies were made by a 'new generation' of filmmakers. Movie after movie were praised by critics and played loads of money. It became the norm!<br /><br />Then came United...<br /><br />*MINOR SPOILERS* It's just simply not funny. Not once. Not ever. But the thing is... We THINK its funny. Because we're used to norwegian movies to be funny. Especially with a cast like this with a few really funny comedians. But.. They neither say nor do anything funny! Where's the humor? Show me the humor! Is it the awkward clerk played by Harald Eia? Is it the overacting totally ridiculously unrealistic football coach? Is it the commentaries by Arne Scheie? The movie is just not funny!<br /><br />But thats not my main rant about United. That namely is the predictability. (And it is here I fear that norwegian comedies have come to a standstill since I have seen this in many other movies as well.) All the time you just know its going to end well. All characters are exactly as they are presented in the start of the movie, and everybody gets exactly what they deserve in the end. There's absolutely no room for surprises at all!<br /><br />All in all I can say that I sat with a bad feeling after seeing this movie. It was the one movie that made me realize that we probably need some new blood in norwegian movie making... again!<br /><br />Rating: 1/6\"]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Vectorization and Embedding"
      ],
      "metadata": {
        "id": "66qrvTJajM7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
        "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
        "\n",
        "# Use the default TextVectorization variables\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                    split=\"whitespace\", # how to split tokens\n",
        "                                    ngrams=None, # create groups of n-words?\n",
        "                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ],
      "metadata": {
        "id": "iGQEPzYSm4d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find average number of tokens in text\n",
        "round(sum([len(str(i).split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxYT38PslhjK",
        "outputId": "23994ede-1b02-4c26-c389-522637b95953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum([len(str(i).split()) for i in train_sentences])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phuyUJfapuvA",
        "outputId": "58884a11-6137-43c1-d8e4-6a7a48055caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9239833"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "max_vocab_length = 900000\n",
        "max_length = 231\n",
        "\n",
        "# Use the default variables\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "nHnUZZzWkG7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit to training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "00B4Pc0HqxI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeE3m5-YqtnE",
        "outputId": "43252717-6f7e-4975-e75c-15a9247be102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 231), dtype=int64, numpy=\n",
              "array([[ 215,    4, 6848,    8,   54,  948,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Choose a random sentence from the training dataset and tokenize it\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nVectorized version:\")\n",
        "text_vectorizer([random_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8d1SmOkp9Y3",
        "outputId": "2f4c7e38-7b00-4747-9751-738eaa915110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "[\"Jason Lee does his best to bring fun to a silly situation, but the movie just fails to make a connect. <br /><br />Perhaps because Julia Stiles character seems awkward as the conniving and sexy soon to be cousin-in-law. <br /><br />Maybe it is because she and Selma Blair's characters should have been cast the opposite way. (Selma Blair seems more conniving than Julia would be).<br /><br />Either way this movie is yet another Hollywood trivialization of a possibly real world situation (that being getting caught with your pants out at your bachelor party not stooping your cousin), which while having promise fails to deliver.<br /><br />There are some laughs to be sure and the cast (even if miscast) do their best with sub grade material which doesn't transcend its raunchy topic. So instead of getting a successful raunch fest (ie Animal House or American Pie) we are left with a middle ground of part humor and part stupidity (ala Meatballs 2 or something).\"]      \n",
            "\n",
            "Vectorized version:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 231), dtype=int64, numpy=\n",
              "array([[ 1612,   955,   122,    25,   116,     6,   728,   245,     6,\n",
              "            4,   670,   873,    19,     2,    18,    40,   996,     6,\n",
              "           94,     4,  3829,    13,    13,   373,    85,  2664,  9647,\n",
              "          109,   181,  1981,    15,     2,  9093,     3,  1233,   528,\n",
              "            6,    27, 69648,    13,    13,   269,     9,     7,    85,\n",
              "           59,     3, 11837, 28830,   102,   140,    26,    75,   179,\n",
              "            2,  1802,    98, 11837,  2937,   181,    53,  9093,    72,\n",
              "         2664,    58,  4565,    13,   351,    98,    11,    18,     7,\n",
              "          242,   155,   369, 57012,     5,     4,   872,   145,   187,\n",
              "          873,    12,   107,   374,   991,    17,   123,  4238,    46,\n",
              "           31,   123,  8005,  1002,    22, 34027,   123,  3014,    62,\n",
              "          132,   255,  2129,   996,     6, 24254,    13,    48,    24,\n",
              "           47,   935,     6,    27,   246,     3,     2,   179,    55,\n",
              "           44,  3187,    77,    64,   116,    17,  3473,  1441,   820,\n",
              "           62,   148, 12489,    30,  8293,  2939,    38,   301,     5,\n",
              "          374,     4,  1104, 37888,  5259,  1726,  1696,   317,    41,\n",
              "          306,  3577,    71,    24,   304,    17,     4,   749,  1537,\n",
              "            5,   171,   462,     3,   171,  2804,  6798, 11088,   279,\n",
              "           41,   138,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words: {top_5_words}\") \n",
        "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0565VyczqAQ8",
        "outputId": "e20f238f-9043-4555-93c3-0fb5aa044cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in vocab: 160802\n",
            "Top 5 most common words: ['', '[UNK]', 'the', 'and', 'a']\n",
            "Bottom 5 least common words: ['0001', '000001', '00000001', '\\x10own', '\\x08\\x08\\x08\\x08a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim = max_vocab_length, #input shape\n",
        "                             output_dim = 128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, initialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\")\n",
        "\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjQ1Li_DrHqk",
        "outputId": "52fba24e-dedd-4283-846b-e35ea59e9879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7fa55eb29810>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text:\\n{random_sentence}\\\n",
        "      \\n\\nEmbedded version:\")\n",
        "\n",
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr6GBXqproVV",
        "outputId": "ea61cf59-fe92-4eab-e071-17237f8a1b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "[\"A wonderful film by Powell and Pressburger, whose work I now want to explore more. The film is about what we perceive as real and what is real, and how the two can be so difficult to distinguish from one another. Beautifully shot and acted, although David Niven doesn't seem to be 27 years old, as his character claims to be. Fun to see a very young Richard Attenborough. This film made me think, while I was watching it, and afterwards.\"]      \n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 231, 128), dtype=float32, numpy=\n",
              "array([[[-0.04364428,  0.02437404, -0.03696011, ..., -0.04763393,\n",
              "          0.02931459,  0.0068561 ],\n",
              "        [ 0.00731655, -0.03667552,  0.04437304, ..., -0.00131422,\n",
              "         -0.04961992, -0.02081001],\n",
              "        [-0.03769413, -0.00289064, -0.03677417, ..., -0.04211248,\n",
              "         -0.03247404,  0.04107854],\n",
              "        ...,\n",
              "        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n",
              "          0.00912381, -0.00024097],\n",
              "        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n",
              "          0.00912381, -0.00024097],\n",
              "        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n",
              "          0.00912381, -0.00024097]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OR Use Universal Sentence Encoder from TF Hub (pretrained embeddings)"
      ],
      "metadata": {
        "id": "yyT5voL8rtZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "metadata": {
        "id": "BuZirbRHssF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[],\n",
        "                                        dtype=tf.string,\n",
        "                                        trainable=False, ## If you want to fine-tune can set trainable=True, False is just feature extraction\n",
        "                                        name=\"USE\")"
      ],
      "metadata": {
        "id": "HmllzOj7xF14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model using Sequential API and encoding layer from USE"
      ],
      "metadata": {
        "id": "eHYmUIZ2WdCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model using Sequential API and encoder layer from USE\n",
        "model_USE = tf.keras.Sequential([\n",
        "                                 sentence_encoder_layer,\n",
        "                                 layers.Dense(64, activation=\"relu\"),\n",
        "                                 layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_USE\")\n",
        "\n",
        "# Compile\n",
        "model_USE.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=tf.keras.optimizers.Adam(),\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "model_USE.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVOqNC-cxb25",
        "outputId": "60949062-f73c-4f54-98dc-77da84e1835a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_USE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " USE (KerasLayer)            (None, 512)               256797824 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                32832     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256,830,721\n",
            "Trainable params: 32,897\n",
            "Non-trainable params: 256,797,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_USE = model_USE.fit(train_sentences,\n",
        "                            train_labels_encoded,\n",
        "                            epochs=5,\n",
        "                            validation_data=(test_sentences, test_labels_encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5c-QKJSx5uC",
        "outputId": "aec4b6b1-8b43-4cf9-80ad-90b33fe1abb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 77s 56ms/step - loss: 0.3570 - accuracy: 0.8440 - val_loss: 0.3213 - val_accuracy: 0.8616\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.3232 - accuracy: 0.8586 - val_loss: 0.3306 - val_accuracy: 0.8552\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 69s 55ms/step - loss: 0.3179 - accuracy: 0.8619 - val_loss: 0.3175 - val_accuracy: 0.8625\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 68s 55ms/step - loss: 0.3132 - accuracy: 0.8632 - val_loss: 0.3118 - val_accuracy: 0.8655\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 0.3077 - accuracy: 0.8662 - val_loss: 0.3135 - val_accuracy: 0.8636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions"
      ],
      "metadata": {
        "id": "7jnjqjPNWk3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make preds\n",
        "model_USE_pred_probs = model_USE.predict(test_sentences)\n",
        "model_USE_pred_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUNnX_2p4QW3",
        "outputId": "94167a4d-5f0a-4c46-9692-c22b2ef8b9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.66395766],\n",
              "       [0.9724653 ],\n",
              "       [0.03029633],\n",
              "       ...,\n",
              "       [0.3844333 ],\n",
              "       [0.04628875],\n",
              "       [0.9414945 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_USE_preds = tf.squeeze(tf.round(model_USE_pred_probs))\n",
        "model_USE_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYnYrApW8tQx",
        "outputId": "8520ca30-0ba2-49f8-818a-f89f433efa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 0., 1., 0., 1., 1., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Results"
      ],
      "metadata": {
        "id": "Gh5IZJOnWnEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_USE_results = calculate_results(test_labels_encoded, model_USE_preds)\n",
        "model_USE_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2GGG9Ef8717",
        "outputId": "dc543210-ffbe-4f6c-ed54-8b7f0c96e23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 86.36,\n",
              " 'f1': 0.8635593341435942,\n",
              " 'precision': 0.8643254352944936,\n",
              " 'recall': 0.8636}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For CNN's for text classification using Conv1D and MaxPooling, see https://dev.mrdbourke.com/tensorflow-deep-learning/08_introduction_to_nlp_in_tensorflow/#convolutional-neural-networks-for-text\n",
        "\n",
        "* For RNN's for text classification using LSTM, GRU, or Bidirectional, see https://dev.mrdbourke.com/tensorflow-deep-learning/08_introduction_to_nlp_in_tensorflow/#recurrent-neural-networks-rnns"
      ],
      "metadata": {
        "id": "6QNuM7Ck9LLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_USE.save(\"/content/drive/MyDrive/Test_Models/model_USE.h5\")"
      ],
      "metadata": {
        "id": "Q194YM7fAROE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}